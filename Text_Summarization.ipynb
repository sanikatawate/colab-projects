{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "c0c2bbf0",
      "metadata": {
        "id": "c0c2bbf0"
      },
      "source": [
        "# 1. Data Preprocessing:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0411044f",
      "metadata": {
        "id": "0411044f"
      },
      "source": [
        "● Load the provided dataset and perform exploratory data analysis.\n",
        "\n",
        "● Preprocess the text data: remove stop words, perform tokenization, stem or lemmatize the words\n",
        "etc.\n",
        "\n",
        "● Split the dataset into a training set and a testing set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb4bf5e4",
      "metadata": {
        "id": "bb4bf5e4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34d082cf-fe77-4942-8870-ba5742f3c453"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.7/210.7 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.4/320.4 kB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m71.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m77.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "#Downloading dependencies/packages\n",
        "!pip install tensorflow -q\n",
        "!pip install rouge -q\n",
        "!pip install textacy -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c9c69c6",
      "metadata": {
        "id": "8c9c69c6"
      },
      "outputs": [],
      "source": [
        "#Importing all libraries\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import re\n",
        "import string\n",
        "import csv\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.utils import pad_sequences\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.layers import LSTM, Dense, Input, Embedding, Concatenate, TimeDistributed, Bidirectional, GRU\n",
        "from keras.models import Model\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.utils import plot_model\n",
        "\n",
        "from rouge import Rouge\n",
        "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2517847",
      "metadata": {
        "id": "b2517847"
      },
      "outputs": [],
      "source": [
        "# Loading the dataset\n",
        "train = pd.read_csv('/content/train.csv', engine='python', on_bad_lines='skip')\n",
        "test= pd.read_csv('/content/test.csv', engine='python', on_bad_lines='skip')\n",
        "validation = pd.read_csv('/content/validation.csv', engine='python', on_bad_lines='skip')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-1o0Q0hutOY0",
        "outputId": "825f04c4-1bc8-4283-fc1f-ec2f8d24c025"
      },
      "id": "-1o0Q0hutOY0",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1951 entries, 0 to 1950\n",
            "Data columns (total 2 columns):\n",
            " #   Column      Non-Null Count  Dtype \n",
            "---  ------      --------------  ----- \n",
            " 0   article     1951 non-null   object\n",
            " 1   highlights  1951 non-null   object\n",
            "dtypes: object(2)\n",
            "memory usage: 30.6+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train = train.head(50)\n",
        "test = test.head(30)\n",
        "validation = validation.head(20)"
      ],
      "metadata": {
        "id": "XKGt37GmtG-o"
      },
      "id": "XKGt37GmtG-o",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train['article'][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "7g8Lv5Xj2GYS",
        "outputId": "bba38a42-c4e1-4512-96e6-838596ac71ca"
      },
      "id": "7g8Lv5Xj2GYS",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"By . Associated Press . PUBLISHED: . 14:11 EST, 25 October 2013 . | . UPDATED: . 15:36 EST, 25 October 2013 . The bishop of the Fargo Catholic Diocese in North Dakota has exposed potentially hundreds of church members in Fargo, Grand Forks and Jamestown to the hepatitis A virus in late September and early October. The state Health Department has issued an advisory of exposure for anyone who attended five churches and took communion. Bishop John Folda (pictured) of the Fargo Catholic Diocese in North Dakota has exposed potentially hundreds of church members in Fargo, Grand Forks and Jamestown to the hepatitis A . State Immunization Program Manager Molly Howell says the risk is low, but officials feel it's important to alert people to the possible exposure. The diocese announced on Monday that Bishop John Folda is taking time off after being diagnosed with hepatitis A. The diocese says he contracted the infection through contaminated food while attending a conference for newly ordained bishops in Italy last month. Symptoms of hepatitis A include fever, tiredness, loss of appetite, nausea and abdominal discomfort. Fargo Catholic Diocese in North Dakota (pictured) is where the bishop is located .\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train['highlights'][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "Xt1k2whvxHtX",
        "outputId": "b1c8c6d0-eb9b-4f53-acd5-8649d0a98624"
      },
      "id": "Xt1k2whvxHtX",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Bishop John Folda, of North Dakota, is taking time off after being diagnosed .\\nHe contracted the infection through contaminated food in Italy .\\nChurch members in Fargo, Grand Forks and Jamestown could have been exposed .'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a191ac0",
      "metadata": {
        "id": "0a191ac0"
      },
      "outputs": [],
      "source": [
        "# Dropping unnecessary columns\n",
        "train = train.drop(['id'], axis=1)\n",
        "test = test.drop(['id'], axis=1)\n",
        "validation = validation.drop(['id'], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function for cleaning the data\n",
        "from textacy import preprocessing as tprep\n",
        "\n",
        "process = tprep.make_pipeline(\n",
        "    tprep.replace.emails,\n",
        "    tprep.replace.emojis,\n",
        "    tprep.replace.urls,\n",
        "    tprep.replace.phone_numbers,\n",
        "    tprep.replace.hashtags,\n",
        "    tprep.replace.currency_symbols,\n",
        "    lambda text: re.sub(r\"\\n\", \" \", text),\n",
        "    tprep.remove.html_tags,\n",
        "    tprep.remove.brackets,\n",
        "    # tprep.remove.punctuation,\n",
        "    tprep.normalize.hyphenated_words,\n",
        "    tprep.normalize.quotation_marks,\n",
        "    tprep.normalize.unicode,\n",
        "    tprep.normalize.bullet_points,\n",
        "    tprep.normalize.whitespace,\n",
        ")\n",
        "\n",
        "train['article'] = train['article'].apply(process)\n",
        "test['article'] = test['article'].apply(process)\n",
        "validation['article'] = validation['article'].apply(process)"
      ],
      "metadata": {
        "id": "JGHWZEuEyzLD"
      },
      "id": "JGHWZEuEyzLD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b22b0485",
      "metadata": {
        "id": "b22b0485",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0c4fc1e-c3c0-4293-fc44-3f239f49ae22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Function for cleaning the data\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def clean_article(para):\n",
        "    para = para.lower()\n",
        "    para = re.sub('[^a-zA-Z]', ' ', para)\n",
        "\n",
        "    all_stopwords = stopwords.words('english')\n",
        "    para = word_tokenize(para)\n",
        "    para = [str(nlp(word)) for word in para if not word in set(stopwords.words('english'))]\n",
        "    para = ' '.join(para)\n",
        "    return para\n",
        "\n",
        "train['article'] = train['article'].apply(clean_article)\n",
        "test['article'] = test['article'].apply(clean_article)\n",
        "validation['article'] = validation['article'].apply(clean_article)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_highlights(para):\n",
        "    para = para.lower()\n",
        "    para = re.sub('[^a-zA-Z]', '', para)\n",
        "    return para\n",
        "\n",
        "train['highlights'] = train['highlights'].apply(clean_highlights)\n",
        "test['highlights'] = test['highlights'].apply(clean_highlights)\n",
        "validation['highlights'] = validation['highlights'].apply(clean_highlights)"
      ],
      "metadata": {
        "id": "5flsmi1m6jMi"
      },
      "id": "5flsmi1m6jMi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "374d1755",
      "metadata": {
        "id": "374d1755",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "765a6480-d40e-4fc1-f8cb-e83d36934236"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Article Length: 2751\n",
            "Average Summary Length: 306\n"
          ]
        }
      ],
      "source": [
        "# Text Length Analysis\n",
        "import math\n",
        "\n",
        "article_lengths = [len(article) for article in train['article']]\n",
        "highlights_lengths = [len(summary) for summary in train['highlights']]\n",
        "\n",
        "avg_article_length = math.floor( sum(article_lengths) / len(article_lengths))\n",
        "avg_highlights_length = math.floor( sum(highlights_lengths) / len(highlights_lengths))\n",
        "\n",
        "print(f\"Average Article Length: {avg_article_length}\")\n",
        "print(f\"Average Summary Length: {avg_highlights_length}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89ef2600",
      "metadata": {
        "id": "89ef2600"
      },
      "outputs": [],
      "source": [
        "# Split the dataset into article and highlights\n",
        "X_train = train['article']\n",
        "y_train = train['highlights']\n",
        "\n",
        "X_test = test['article']\n",
        "y_test = test['highlights']\n",
        "\n",
        "X_val = validation['article']\n",
        "y_val = validation['highlights']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ecb0343",
      "metadata": {
        "id": "9ecb0343"
      },
      "outputs": [],
      "source": [
        "# Maximum sequence lengths\n",
        "maxlen_articles = max(len(sequence) for sequence in X_train)\n",
        "maxlen_highlights = max(len(sequence) for sequence in y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1c6b37b",
      "metadata": {
        "id": "e1c6b37b"
      },
      "outputs": [],
      "source": [
        "# Tokenize the articles and highlights\n",
        "tokenizer_articles = Tokenizer()\n",
        "tokenizer_articles.fit_on_texts(X_train)\n",
        "X_train_seq = tokenizer_articles.texts_to_sequences(X_train)\n",
        "X_test_seq = tokenizer_articles.texts_to_sequences(X_test)\n",
        "X_val_seq = tokenizer_articles.texts_to_sequences(X_val)\n",
        "\n",
        "tokenizer_highlights = Tokenizer()\n",
        "tokenizer_highlights.fit_on_texts(y_train)\n",
        "y_train_seq = tokenizer_highlights.texts_to_sequences(y_train)\n",
        "y_test_seq = tokenizer_highlights.texts_to_sequences(y_test)\n",
        "y_test_val = tokenizer_highlights.texts_to_sequences(y_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4e6e697",
      "metadata": {
        "id": "e4e6e697"
      },
      "outputs": [],
      "source": [
        "# Padding sequences\n",
        "X_train_padded = pad_sequences(X_train_seq, maxlen=maxlen_articles, padding='post')\n",
        "X_test_padded = pad_sequences(X_test_seq, maxlen=maxlen_articles, padding='post')\n",
        "X_val_padded = pad_sequences(X_val_seq, maxlen=maxlen_articles, padding='post')\n",
        "\n",
        "y_train_padded = pad_sequences(y_train_seq, maxlen=maxlen_highlights, padding='post')\n",
        "y_test_padded = pad_sequences(y_test_seq, maxlen=maxlen_highlights, padding='post')\n",
        "y_val_padded = pad_sequences(y_val_seq, maxlen=maxlen_highlights, padding='post')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "878dd5bf",
      "metadata": {
        "id": "878dd5bf"
      },
      "outputs": [],
      "source": [
        "# Data-analysis\n",
        "print(\"Training Sequence\", train_x.shape)\n",
        "print('Target Values Shape', train_y.shape)\n",
        "print('Test Sequence', test_x.shape)\n",
        "print('Target Test Shape', test_y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb915133",
      "metadata": {
        "id": "cb915133"
      },
      "outputs": [],
      "source": [
        "# Vocabulary size\n",
        "vocab_size_articles = len(tokenizer_articles.word_index) + 1\n",
        "vocab_size_highlights = len(tokenizer_highlights.word_index) + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd21b434",
      "metadata": {
        "id": "bd21b434"
      },
      "outputs": [],
      "source": [
        "embedding_dim = 100"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f0e81908",
      "metadata": {
        "id": "f0e81908"
      },
      "source": [
        "# 2. Model Building:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "682f3923",
      "metadata": {
        "id": "682f3923"
      },
      "source": [
        "● Develop a sequence-to-sequence model for text summarization.\n",
        "Explore architectures like LSTM(Long Short-Term Memory),\n",
        "GRU (Gated Recurrent Units),\n",
        "or even transformer-based modelslike\n",
        "BERT (Bidirectional Encoder Representations from Transformers),\n",
        "GPT (Generative Pre-trained Transformer), etc.\n",
        "\n",
        "● Train the model on the training set and tune the hyperparameters for optimal performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d5afdf0",
      "metadata": {
        "id": "6d5afdf0"
      },
      "outputs": [],
      "source": [
        "# Encoder input\n",
        "latent_dim = 128\n",
        "encoder_input = Input(shape=(maxlen_articles,))\n",
        "encoder_embedding = Embedding(vocab_size_articles, embedding_dim, trainable=True)(encoder_input)\n",
        "encoder_lstm = LSTM(latent_dim*2, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "544b3f4d",
      "metadata": {
        "id": "544b3f4d"
      },
      "outputs": [],
      "source": [
        "# Decoder input\n",
        "decoder_inputs = Input(shape=(maxlen_highlights,))\n",
        "decoder_embedding = Embedding(vocab_size_highlights, embedding_dim, trainable=False)(decoder_inputs)\n",
        "decoder_lstm = LSTM(latent_dim*2, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=[state_h, state_c])\n",
        "decoder_dense = Dense(vocab_size_highlights, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aff6cb84",
      "metadata": {
        "id": "aff6cb84"
      },
      "outputs": [],
      "source": [
        "# Define the model\n",
        "model = Model([encoder_input, decoder_inputs], decoder_outputs)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52c4c0fa",
      "metadata": {
        "id": "52c4c0fa"
      },
      "outputs": [],
      "source": [
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1fd012e2",
      "metadata": {
        "id": "1fd012e2"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "model.fit([X_train_padded, y_train_padded[:, :-1]], y_train_padded[:, 1:], epochs=10, batch_size=128,\n",
        "          validation_data=([X_val_padded, y_val_padded[:, :-1]], y_val_padded[:, 1:]), callbacks=[early_stop])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d7507e4",
      "metadata": {
        "id": "4d7507e4"
      },
      "outputs": [],
      "source": [
        "# Generate summaries for test data\n",
        "encoder_model = Model(inputs= encoder_input, outputs=[state_h, state_c])\n",
        "\n",
        "decoder_state_input_h = Input(shape=(latent_dim*2,))\n",
        "decoder_state_input_c = Input(shape=(latent_dim*2,))\n",
        "\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "decoder_outputs, state_h, state_c = decoder_lstm(decoder_embedding, initial_state=decoder_states_inputs)\n",
        "decoder_states = [state_h, state_c]\n",
        "\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "201c9d18",
      "metadata": {
        "id": "201c9d18"
      },
      "outputs": [],
      "source": [
        "# Function to generate summaries\n",
        "def generate_summary(input_seq):\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "    target_seq = np.zeros((1, 1))\n",
        "\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_word = tokenizer_highlights.index_word[sampled_token_index]\n",
        "\n",
        "        if sampled_word != 'end':\n",
        "            decoded_sentence += ' ' + sampled_word\n",
        "\n",
        "        if sampled_word == 'end' or len(decoded_sentence.split()) >= maxlen_highlights:\n",
        "            stop_condition = True\n",
        "\n",
        "        target_seq = np.zeros((1, 1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "        states_value = [h, c]\n",
        "\n",
        "    return decoded_sentence.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "407710e2",
      "metadata": {
        "id": "407710e2"
      },
      "outputs": [],
      "source": [
        "# Generate summaries for test samples\n",
        "for i in range(10):\n",
        "    article_seq = X_test_padded[i:i+1]\n",
        "    summary = generate_summary(article_seq)\n",
        "    print(f\"Article: {X_test[i]}\")\n",
        "    print(f\"Summary: {summary}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14bf4e36",
      "metadata": {
        "id": "14bf4e36"
      },
      "source": [
        "# 3. Evaluation:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5cb832d0",
      "metadata": {
        "id": "5cb832d0"
      },
      "source": [
        "● Test the model on the testing set.\n",
        "\n",
        "● Evaluate the model using appropriate metrics such as ROUGE (Recall-Oriented Understudy for\n",
        "Gisting Evaluation) scores, BLEU (Bilingual Evaluation Understudy) scores etc.\n",
        "\n",
        "● Analyze the performance and discuss any limitations and potential improvements.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54058a40",
      "metadata": {
        "id": "54058a40"
      },
      "outputs": [],
      "source": [
        "# Calculate ROUGE scores\n",
        "rouge = Rouge()\n",
        "scores = rouge.get_scores(decoded_sentence, test_data.highlights, avg=True)\n",
        "\n",
        "print(\"ROUGE scores:\")\n",
        "for metric, values in scores.items():\n",
        "    print(f\"{metric}: {values['f']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff360589",
      "metadata": {
        "id": "ff360589"
      },
      "outputs": [],
      "source": [
        "# Calculate BLEU score\n",
        "references_tokens = [reference.split() for reference in test_data.highlights]\n",
        "candidates_tokens = [candidate.split() for candidate in decoded_sentence]\n",
        "bleu_score = corpus_bleu(references_tokens, candidates_tokens)\n",
        "\n",
        "print(f\"\\nBLEU score: {bleu_score}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "14bf4e36"
      ],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}